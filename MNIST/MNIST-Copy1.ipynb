{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15898d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-10 22:00:24.852632: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-10 22:00:24.852665: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf \n",
    "# Common imports\n",
    "import os\n",
    "\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Flatten\n",
    "from tensorflow.python.keras.layers import Dense, Dropout\n",
    "from tensorflow.python.keras.layers import Activation\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.keras import backend_config\n",
    "from tensorflow.python.keras.optimizer_v2 import learning_rate_schedule\n",
    "from tensorflow.python.keras.optimizer_v2 import optimizer_v2\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import state_ops\n",
    "from tensorflow.python.ops import variables as tf_variables\n",
    "from tensorflow.python.util.tf_export import keras_export\n",
    "\n",
    "import scipy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2869fb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter(n):\n",
    "    alpha = []\n",
    "    for i in range(n):\n",
    "    \talpha.append(1.2 - 0.05**(i+1)) #0.8 \n",
    "    return alpha\n",
    "\n",
    "def parameter1(n):\n",
    "    alpha = []\n",
    "    for i in range(n):\n",
    "        alpha.append(1.4 - 0.005**(i+1)) # 0.8\n",
    "    beta = []\n",
    "    for i in range(n):\n",
    "        beta.append(0.9 - 0.005**(i+1)) #0.8\n",
    "    return alpha, beta\n",
    "\n",
    "def summation(n):\n",
    "    summ = 0\n",
    "    alpha = parameter(n)\n",
    "    for i in range(n):\n",
    "        summ += alpha[i]\n",
    "    return summ\n",
    "\n",
    "def Fisher(n):\n",
    "    FIM = []\n",
    "    alpha = parameter(n)\n",
    "    for i in range(n):\n",
    "        Row = []\n",
    "        for j in range(n):\n",
    "            if i == j: \n",
    "                Row.append(special.polygamma(1, alpha[i]) - special.polygamma(1, summation(n)))\n",
    "            else:\n",
    "                Row.append(- special.polygamma(1, summation(n)))\n",
    "        FIM.append(Row)\n",
    "    FIM = numpy.array(FIM)\n",
    "    FIM = numpy.linalg.inv(FIM)\n",
    "    return FIM\n",
    "\n",
    "def Fisher2(n):\n",
    "    FIM = []\n",
    "    for i in range(n):\n",
    "        Zero = []\n",
    "        for j in range(n):\n",
    "            Zero.append(0)\n",
    "        FIM.append(Zero)\n",
    "    alpha, beta = parameter1(n)\n",
    "    for i in range(0,n,2):\n",
    "        FIM[i][i] = special.polygamma(1, alpha[i]) - special.polygamma(1, alpha[i]+beta[i])\n",
    "        FIM[i+1][i] = - special.polygamma(1, alpha[i]+beta[i])\n",
    "        FIM[i][i+1] = - special.polygamma(1, alpha[i]+beta[i])\n",
    "        FIM[i+1][i+1] = special.polygamma(1, beta[i]) - special.polygamma(1, alpha[i]+beta[i])\n",
    "    FIM = numpy.array(FIM)\n",
    "    FIM = numpy.linalg.inv(FIM)\n",
    "    return FIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c6513d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Riemann_Liouville_derivative(gradient, alpha, kappa, variable):\n",
    "    RL_gradient = np.array([])\n",
    "    for i in range(len(gradient)):\n",
    "        RL_gradient.append(\n",
    "            1/scipy.special.gamma(len(gradient)-alpha)*scipy.misc.derivative(\n",
    "                scipy.integrate.quad(\n",
    "                    lambda tau: variable[i]/(tau-variable[i])**(alpha-kappa+1),0,variable[i])\n",
    "                , n = kappa)\n",
    "            )\n",
    "    return RL_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65ea47a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Caputo_derivative(gradient, alpha, kappa, variable):\n",
    "    C_gradient = np.array([])\n",
    "    for i in range(len(gradient)):\n",
    "        C_gradient.append(\n",
    "            1/scipy.special.gamma(len(gradient)-alpha)*scipy.integrate.quad(\n",
    "                lambda tau: scipy.misc.derivative(variable[i], n = kappa)/(tau-variable[i])**(alpha-kappa+1),0,variable[i])\n",
    "            )\n",
    "    return C_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47aa37eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Grunwald_Letnikov_derivative(gradient, h, alpha, kappa, variable):\n",
    "    GL_gradient = np.array([])\n",
    "    for i in range(len(gradient)):\n",
    "        summ = 0\n",
    "        for j in range(len(gradient)):\n",
    "            summ += (-1)**k * scipy.special.gamma(alpha+1)*variable[j]/(scipy.special.gamma(kappa+1)*scipy.special.gamma(alpha-kappa+1))\n",
    "        GL_gradient.append(\n",
    "            1/h**alpha * summ\n",
    "        ) \n",
    "    return GL_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1848010",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras_export(\"keras.optimizers.SGD\")\n",
    "class SGD(optimizer_v2.OptimizerV2):\n",
    "\n",
    "  _HAS_AGGREGATE_GRAD = True\n",
    "\n",
    "  def __init__(self,\n",
    "               learning_rate=0.01,\n",
    "               momentum=0.0,\n",
    "               nesterov=False,\n",
    "               name=\"SGD\",\n",
    "               **kwargs):\n",
    "    super(SGD, self).__init__(name, **kwargs)\n",
    "    self._set_hyper(\"learning_rate\", kwargs.get(\"lr\", learning_rate))\n",
    "    self._set_hyper(\"decay\", self._initial_decay)\n",
    "\n",
    "    self._momentum = False\n",
    "    if isinstance(momentum, tf.Tensor) or callable(momentum) or momentum > 0:\n",
    "      self._momentum = True\n",
    "    if isinstance(momentum, (int, float)) and (momentum < 0 or momentum > 1):\n",
    "      raise ValueError(f\"`momentum` must be between [0, 1]. Received: \"\n",
    "                       f\"momentum={momentum} (of type {type(momentum)}).\")\n",
    "    self._set_hyper(\"momentum\", momentum)\n",
    "\n",
    "    self.nesterov = nesterov\n",
    "\n",
    "  def _create_slots(self, var_list):\n",
    "    if self._momentum:\n",
    "      for var in var_list:\n",
    "        self.add_slot(var, \"momentum\")\n",
    "\n",
    "  def _prepare_local(self, var_device, var_dtype, apply_state):\n",
    "    super(SGD, self)._prepare_local(var_device, var_dtype, apply_state)\n",
    "    apply_state[(var_device, var_dtype)][\"momentum\"] = tf.identity(\n",
    "        self._get_hyper(\"momentum\", var_dtype))\n",
    "\n",
    "  def _resource_apply_dense(self, grad, var, apply_state=None):\n",
    "    var_device, var_dtype = var.device, var.dtype.base_dtype\n",
    "    coefficients = ((apply_state or {}).get((var_device, var_dtype))\n",
    "                    or self._fallback_apply_state(var_device, var_dtype))\n",
    "\n",
    "    if self._momentum:\n",
    "      momentum_var = self.get_slot(var, \"momentum\")\n",
    "      return tf.raw_ops.ResourceApplyKerasMomentum(\n",
    "          var=var.handle,\n",
    "          accum=momentum_var.handle,\n",
    "          lr=coefficients[\"lr_t\"],\n",
    "          grad=grad,\n",
    "          momentum=coefficients[\"momentum\"],\n",
    "          use_locking=self._use_locking,\n",
    "          use_nesterov=self.nesterov)\n",
    "    else:\n",
    "      return tf.raw_ops.ResourceApplyGradientDescent(\n",
    "          var=var.handle,\n",
    "          alpha=coefficients[\"lr_t\"],\n",
    "          delta=grad,\n",
    "          use_locking=self._use_locking)\n",
    "\n",
    "  def _resource_apply_sparse_duplicate_indices(self, grad, var, indices,\n",
    "                                               **kwargs):\n",
    "    if self._momentum:\n",
    "      return super(SGD, self)._resource_apply_sparse_duplicate_indices(\n",
    "          grad, var, indices, **kwargs)\n",
    "    else:\n",
    "      var_device, var_dtype = var.device, var.dtype.base_dtype\n",
    "      coefficients = (kwargs.get(\"apply_state\", {}).get((var_device, var_dtype))\n",
    "                      or self._fallback_apply_state(var_device, var_dtype))\n",
    "\n",
    "      return tf.raw_ops.ResourceScatterAdd(\n",
    "          resource=var.handle,\n",
    "          indices=indices,\n",
    "          updates=-grad * coefficients[\"lr_t\"])\n",
    "\n",
    "  def _resource_apply_sparse(self, grad, var, indices, apply_state=None):\n",
    "    # This method is only needed for momentum optimization.\n",
    "    var_device, var_dtype = var.device, var.dtype.base_dtype\n",
    "    coefficients = ((apply_state or {}).get((var_device, var_dtype))\n",
    "                    or self._fallback_apply_state(var_device, var_dtype))\n",
    "\n",
    "    momentum_var = self.get_slot(var, \"momentum\")\n",
    "    return tf.raw_ops.ResourceSparseApplyKerasMomentum(\n",
    "        var=var.handle,\n",
    "        accum=momentum_var.handle,\n",
    "        lr=coefficients[\"lr_t\"],\n",
    "        grad=grad,\n",
    "        indices=indices,\n",
    "        momentum=coefficients[\"momentum\"],\n",
    "        use_locking=self._use_locking,\n",
    "        use_nesterov=self.nesterov)\n",
    "\n",
    "  def get_config(self):\n",
    "    config = super(SGD, self).get_config()\n",
    "    config.update({\n",
    "        \"learning_rate\": self._serialize_hyperparameter(\"learning_rate\"),\n",
    "        \"decay\": self._initial_decay,\n",
    "        \"momentum\": self._serialize_hyperparameter(\"momentum\"),\n",
    "        \"nesterov\": self.nesterov,\n",
    "    })\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "675ae555",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLFGD(optimizer_v2.OptimizerV2):\n",
    "  _HAS_AGGREGATE_GRAD = True\n",
    "\n",
    "  def __init__(self,\n",
    "               learning_rate=0.01,\n",
    "               momentum=0.0,\n",
    "               nesterov=False,\n",
    "               name=\"RLFGD\",\n",
    "               **kwargs):\n",
    "    super(RLFGD, self).__init__(name, **kwargs)\n",
    "    self._set_hyper(\"learning_rate\", kwargs.get(\"lr\", learning_rate))\n",
    "    self._set_hyper(\"decay\", self._initial_decay)\n",
    "\n",
    "    self._momentum = False\n",
    "    if isinstance(momentum, tf.Tensor) or callable(momentum) or momentum > 0:\n",
    "      self._momentum = True\n",
    "    if isinstance(momentum, (int, float)) and (momentum < 0 or momentum > 1):\n",
    "      raise ValueError(f\"`momentum` must be between [0, 1]. Received: \"\n",
    "                       f\"momentum={momentum} (of type {type(momentum)}).\")\n",
    "    self._set_hyper(\"momentum\", momentum)\n",
    "\n",
    "    self.nesterov = nesterov\n",
    "\n",
    "  def _create_slots(self, var_list):\n",
    "    if self._momentum:\n",
    "      for var in var_list:\n",
    "        self.add_slot(var, \"momentum\")\n",
    "\n",
    "  def _prepare_local(self, var_device, var_dtype, apply_state):\n",
    "    super(RLFGD, self)._prepare_local(var_device, var_dtype, apply_state)\n",
    "    apply_state[(var_device, var_dtype)][\"momentum\"] = tf.identity(\n",
    "        self._get_hyper(\"momentum\", var_dtype))\n",
    "\n",
    "  def _resource_apply_dense(self, grad, var, apply_state=None):\n",
    "    var_device, var_dtype = var.device, var.dtype.base_dtype\n",
    "    coefficients = ((apply_state or {}).get((var_device, var_dtype))\n",
    "                    or self._fallback_apply_state(var_device, var_dtype))\n",
    "\n",
    "    if self._momentum:\n",
    "      momentum_var = self.get_slot(var, \"momentum\")\n",
    "      return tf.raw_ops.ResourceApplyKerasMomentum(\n",
    "          var=var.handle,\n",
    "          accum=momentum_var.handle,\n",
    "          lr=coefficients[\"lr_t\"],\n",
    "          grad=grad,\n",
    "          momentum=coefficients[\"momentum\"],\n",
    "          use_locking=self._use_locking,\n",
    "          use_nesterov=self.nesterov)\n",
    "    else:\n",
    "      return tf.raw_ops.ResourceApplyGradientDescent(\n",
    "          var=var.handle,\n",
    "          alpha=coefficients[\"lr_t\"],\n",
    "          delta=grad,\n",
    "          use_locking=self._use_locking)\n",
    "\n",
    "  def _resource_apply_sparse_duplicate_indices(self, grad, var, indices,\n",
    "                                               **kwargs):\n",
    "    if self._momentum:\n",
    "      return super(RLFGD, self)._resource_apply_sparse_duplicate_indices(\n",
    "          grad, var, indices, **kwargs)\n",
    "    else:\n",
    "      var_device, var_dtype = var.device, var.dtype.base_dtype\n",
    "      coefficients = (kwargs.get(\"apply_state\", {}).get((var_device, var_dtype))\n",
    "                      or self._fallback_apply_state(var_device, var_dtype))\n",
    "\n",
    "      return tf.raw_ops.ResourceScatterAdd(\n",
    "          resource=var.handle,\n",
    "          indices=indices,\n",
    "          updates=np.dot(Fisher(len(grad)),Riemann_Liouville_derivative(grad, 7.8, 2, var).transpose()) * coefficients[\"lr_t\"])\n",
    "          #updates=-Riemann_Liouville_derivative(grad, 8.8, 4, var) * coefficients[\"lr_t\"])\n",
    "          #updates=-grad * coefficients[\"lr_t\"])\n",
    "\n",
    "  def _resource_apply_sparse(self, grad, var, indices, apply_state=None):\n",
    "    # This method is only needed for momentum optimization.\n",
    "    var_device, var_dtype = var.device, var.dtype.base_dtype\n",
    "    coefficients = ((apply_state or {}).get((var_device, var_dtype))\n",
    "                    or self._fallback_apply_state(var_device, var_dtype))\n",
    "\n",
    "    momentum_var = self.get_slot(var, \"momentum\")\n",
    "    return tf.raw_ops.ResourceSparseApplyKerasMomentum(\n",
    "        var=var.handle,\n",
    "        accum=momentum_var.handle,\n",
    "        lr=coefficients[\"lr_t\"],\n",
    "        grad=grad,\n",
    "        indices=indices,\n",
    "        momentum=coefficients[\"momentum\"],\n",
    "        use_locking=self._use_locking,\n",
    "        use_nesterov=self.nesterov)\n",
    "\n",
    "  def get_config(self):\n",
    "    config = super(RLFGD, self).get_config()\n",
    "    config.update({\n",
    "        \"learning_rate\": self._serialize_hyperparameter(\"learning_rate\"),\n",
    "        \"decay\": self._initial_decay,\n",
    "        \"momentum\": self._serialize_hyperparameter(\"momentum\"),\n",
    "        \"nesterov\": self.nesterov,\n",
    "    })\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd2cd5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFGD(optimizer_v2.OptimizerV2):\n",
    "  _HAS_AGGREGATE_GRAD = True\n",
    "\n",
    "  def __init__(self,\n",
    "               learning_rate=0.01,\n",
    "               momentum=0.0,\n",
    "               nesterov=False,\n",
    "               name=\"CFGD\",\n",
    "               **kwargs):\n",
    "    super(CFGD, self).__init__(name, **kwargs)\n",
    "    self._set_hyper(\"learning_rate\", kwargs.get(\"lr\", learning_rate))\n",
    "    self._set_hyper(\"decay\", self._initial_decay)\n",
    "\n",
    "    self._momentum = False\n",
    "    if isinstance(momentum, tf.Tensor) or callable(momentum) or momentum > 0:\n",
    "      self._momentum = True\n",
    "    if isinstance(momentum, (int, float)) and (momentum < 0 or momentum > 1):\n",
    "      raise ValueError(f\"`momentum` must be between [0, 1]. Received: \"\n",
    "                       f\"momentum={momentum} (of type {type(momentum)}).\")\n",
    "    self._set_hyper(\"momentum\", momentum)\n",
    "\n",
    "    self.nesterov = nesterov\n",
    "\n",
    "  def _create_slots(self, var_list):\n",
    "    if self._momentum:\n",
    "      for var in var_list:\n",
    "        self.add_slot(var, \"momentum\")\n",
    "\n",
    "  def _prepare_local(self, var_device, var_dtype, apply_state):\n",
    "    super(CFGD, self)._prepare_local(var_device, var_dtype, apply_state)\n",
    "    apply_state[(var_device, var_dtype)][\"momentum\"] = tf.identity(\n",
    "        self._get_hyper(\"momentum\", var_dtype))\n",
    "\n",
    "  def _resource_apply_dense(self, grad, var, apply_state=None):\n",
    "    var_device, var_dtype = var.device, var.dtype.base_dtype\n",
    "    coefficients = ((apply_state or {}).get((var_device, var_dtype))\n",
    "                    or self._fallback_apply_state(var_device, var_dtype))\n",
    "\n",
    "    if self._momentum:\n",
    "      momentum_var = self.get_slot(var, \"momentum\")\n",
    "      return tf.raw_ops.ResourceApplyKerasMomentum(\n",
    "          var=var.handle,\n",
    "          accum=momentum_var.handle,\n",
    "          lr=coefficients[\"lr_t\"],\n",
    "          grad=grad,\n",
    "          momentum=coefficients[\"momentum\"],\n",
    "          use_locking=self._use_locking,\n",
    "          use_nesterov=self.nesterov)\n",
    "    else:\n",
    "      return tf.raw_ops.ResourceApplyGradientDescent(\n",
    "          var=var.handle,\n",
    "          alpha=coefficients[\"lr_t\"],\n",
    "          delta=grad,\n",
    "          use_locking=self._use_locking)\n",
    "\n",
    "  def _resource_apply_sparse_duplicate_indices(self, grad, var, indices,\n",
    "                                               **kwargs):\n",
    "    if self._momentum:\n",
    "      return super(CFGD, self)._resource_apply_sparse_duplicate_indices(\n",
    "          grad, var, indices, **kwargs)\n",
    "    else:\n",
    "      var_device, var_dtype = var.device, var.dtype.base_dtype\n",
    "      coefficients = (kwargs.get(\"apply_state\", {}).get((var_device, var_dtype))\n",
    "                      or self._fallback_apply_state(var_device, var_dtype))\n",
    "\n",
    "      return tf.raw_ops.ResourceScatterAdd(\n",
    "          resource=var.handle,\n",
    "          indices=indices,\n",
    "          updates=np.dot(Fisher(len(grad)),Caputo_derivative(grad, 9.8, 2, var).transpose()) * coefficients[\"lr_t\"])\n",
    "          #updates=-Caputo_derivative(grad, 8.8, 4, var) * coefficients[\"lr_t\"])\n",
    "          #updates=-grad * coefficients[\"lr_t\"])\n",
    "\n",
    "  def _resource_apply_sparse(self, grad, var, indices, apply_state=None):\n",
    "    # This method is only needed for momentum optimization.\n",
    "    var_device, var_dtype = var.device, var.dtype.base_dtype\n",
    "    coefficients = ((apply_state or {}).get((var_device, var_dtype))\n",
    "                    or self._fallback_apply_state(var_device, var_dtype))\n",
    "\n",
    "    momentum_var = self.get_slot(var, \"momentum\")\n",
    "    return tf.raw_ops.ResourceSparseApplyKerasMomentum(\n",
    "        var=var.handle,\n",
    "        accum=momentum_var.handle,\n",
    "        lr=coefficients[\"lr_t\"],\n",
    "        grad=grad,\n",
    "        indices=indices,\n",
    "        momentum=coefficients[\"momentum\"],\n",
    "        use_locking=self._use_locking,\n",
    "        use_nesterov=self.nesterov)\n",
    "\n",
    "  def get_config(self):\n",
    "    config = super(CFGD, self).get_config()\n",
    "    config.update({\n",
    "        \"learning_rate\": self._serialize_hyperparameter(\"learning_rate\"),\n",
    "        \"decay\": self._initial_decay,\n",
    "        \"momentum\": self._serialize_hyperparameter(\"momentum\"),\n",
    "        \"nesterov\": self.nesterov,\n",
    "    })\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0d723c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLFGD(optimizer_v2.OptimizerV2):\n",
    "  _HAS_AGGREGATE_GRAD = True\n",
    "\n",
    "  def __init__(self,\n",
    "               learning_rate=0.01,\n",
    "               momentum=0.0,\n",
    "               nesterov=False,\n",
    "               name=\"GLFGD\",\n",
    "               **kwargs):\n",
    "    super(GLFGD, self).__init__(name, **kwargs)\n",
    "    self._set_hyper(\"learning_rate\", kwargs.get(\"lr\", learning_rate))\n",
    "    self._set_hyper(\"decay\", self._initial_decay)\n",
    "\n",
    "    self._momentum = False\n",
    "    if isinstance(momentum, tf.Tensor) or callable(momentum) or momentum > 0:\n",
    "      self._momentum = True\n",
    "    if isinstance(momentum, (int, float)) and (momentum < 0 or momentum > 1):\n",
    "      raise ValueError(f\"`momentum` must be between [0, 1]. Received: \"\n",
    "                       f\"momentum={momentum} (of type {type(momentum)}).\")\n",
    "    self._set_hyper(\"momentum\", momentum)\n",
    "\n",
    "    self.nesterov = nesterov\n",
    "\n",
    "  def _create_slots(self, var_list):\n",
    "    if self._momentum:\n",
    "      for var in var_list:\n",
    "        self.add_slot(var, \"momentum\")\n",
    "\n",
    "  def _prepare_local(self, var_device, var_dtype, apply_state):\n",
    "    super(GLFGD, self)._prepare_local(var_device, var_dtype, apply_state)\n",
    "    apply_state[(var_device, var_dtype)][\"momentum\"] = tf.identity(\n",
    "        self._get_hyper(\"momentum\", var_dtype))\n",
    "\n",
    "  def _resource_apply_dense(self, grad, var, apply_state=None):\n",
    "    var_device, var_dtype = var.device, var.dtype.base_dtype\n",
    "    coefficients = ((apply_state or {}).get((var_device, var_dtype))\n",
    "                    or self._fallback_apply_state(var_device, var_dtype))\n",
    "\n",
    "    if self._momentum:\n",
    "      momentum_var = self.get_slot(var, \"momentum\")\n",
    "      return tf.raw_ops.ResourceApplyKerasMomentum(\n",
    "          var=var.handle,\n",
    "          accum=momentum_var.handle,\n",
    "          lr=coefficients[\"lr_t\"],\n",
    "          grad=grad,\n",
    "          momentum=coefficients[\"momentum\"],\n",
    "          use_locking=self._use_locking,\n",
    "          use_nesterov=self.nesterov)\n",
    "    else:\n",
    "      return tf.raw_ops.ResourceApplyGradientDescent(\n",
    "          var=var.handle,\n",
    "          alpha=coefficients[\"lr_t\"],\n",
    "          delta=grad,\n",
    "          use_locking=self._use_locking)\n",
    "\n",
    "  def _resource_apply_sparse_duplicate_indices(self, grad, var, indices,\n",
    "                                               **kwargs):\n",
    "    if self._momentum:\n",
    "      return super(GLFGD, self)._resource_apply_sparse_duplicate_indices(\n",
    "          grad, var, indices, **kwargs)\n",
    "    else:\n",
    "      var_device, var_dtype = var.device, var.dtype.base_dtype\n",
    "      coefficients = (kwargs.get(\"apply_state\", {}).get((var_device, var_dtype))\n",
    "                      or self._fallback_apply_state(var_device, var_dtype))\n",
    "\n",
    "      return tf.raw_ops.ResourceScatterAdd(\n",
    "          resource=var.handle,\n",
    "          indices=indices,\n",
    "          #updates=-Grunwald_Letnikov_derivative(grad, 0.01, 6.8, 4, var) * coefficients[\"lr_t\"])\n",
    "          updates=np.dot(Fisher(len(grad)),Grunwald_Letnikov_derivative(grad, 0.001, 7.8, 2, var).transpose()) * coefficients[\"lr_t\"])          \n",
    "          #updates=-grad * coefficients[\"lr_t\"])\n",
    "\n",
    "  def _resource_apply_sparse(self, grad, var, indices, apply_state=None):\n",
    "    # This method is only needed for momentum optimization.\n",
    "    var_device, var_dtype = var.device, var.dtype.base_dtype\n",
    "    coefficients = ((apply_state or {}).get((var_device, var_dtype))\n",
    "                    or self._fallback_apply_state(var_device, var_dtype))\n",
    "\n",
    "    momentum_var = self.get_slot(var, \"momentum\")\n",
    "    return tf.raw_ops.ResourceSparseApplyKerasMomentum(\n",
    "        var=var.handle,\n",
    "        accum=momentum_var.handle,\n",
    "        lr=coefficients[\"lr_t\"],\n",
    "        grad=grad,\n",
    "        indices=indices,\n",
    "        momentum=coefficients[\"momentum\"],\n",
    "        use_locking=self._use_locking,\n",
    "        use_nesterov=self.nesterov)\n",
    "\n",
    "  def get_config(self):\n",
    "    config = super(GLFGD, self).get_config()\n",
    "    config.update({\n",
    "        \"learning_rate\": self._serialize_hyperparameter(\"learning_rate\"),\n",
    "        \"decay\": self._initial_decay,\n",
    "        \"momentum\": self._serialize_hyperparameter(\"momentum\"),\n",
    "        \"nesterov\": self.nesterov,\n",
    "    })\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eae2b8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras_export('keras.optimizers.Adam')\n",
    "class Adam(optimizer_v2.OptimizerV2):\n",
    "\n",
    "  _HAS_AGGREGATE_GRAD = True\n",
    "\n",
    "  def __init__(self,\n",
    "               learning_rate=0.001,\n",
    "               beta_1=0.9,\n",
    "               beta_2=0.999,\n",
    "               epsilon=1e-7,\n",
    "               amsgrad=False,\n",
    "               name='Adam',\n",
    "               **kwargs):\n",
    "    super(Adam, self).__init__(name, **kwargs)\n",
    "    self._set_hyper('learning_rate', kwargs.get('lr', learning_rate))\n",
    "    self._set_hyper('decay', self._initial_decay)\n",
    "    self._set_hyper('beta_1', beta_1)\n",
    "    self._set_hyper('beta_2', beta_2)\n",
    "    self.epsilon = epsilon or backend_config.epsilon()\n",
    "    self.amsgrad = amsgrad\n",
    "\n",
    "  def _create_slots(self, var_list):\n",
    "    # Create slots for the first and second moments.\n",
    "    # Separate for-loops to respect the ordering of slot variables from v1.\n",
    "    for var in var_list:\n",
    "      self.add_slot(var, 'm')\n",
    "    for var in var_list:\n",
    "      self.add_slot(var, 'v')\n",
    "    if self.amsgrad:\n",
    "      for var in var_list:\n",
    "        self.add_slot(var, 'vhat')\n",
    "\n",
    "  def _prepare_local(self, var_device, var_dtype, apply_state):\n",
    "    super(Adam, self)._prepare_local(var_device, var_dtype, apply_state)\n",
    "\n",
    "    local_step = tf.cast(self.iterations + 1, var_dtype)\n",
    "    beta_1_t = tf.identity(self._get_hyper('beta_1', var_dtype))\n",
    "    beta_2_t = tf.identity(self._get_hyper('beta_2', var_dtype))\n",
    "    beta_1_power = tf.pow(beta_1_t, local_step)\n",
    "    beta_2_power = tf.pow(beta_2_t, local_step)\n",
    "    lr = (apply_state[(var_device, var_dtype)]['lr_t'] *\n",
    "          (tf.sqrt(1 - beta_2_power) / (1 - beta_1_power)))\n",
    "    apply_state[(var_device, var_dtype)].update(\n",
    "        dict(\n",
    "            lr=lr,\n",
    "            epsilon=tf.convert_to_tensor(\n",
    "                self.epsilon, var_dtype),\n",
    "            beta_1_t=beta_1_t,\n",
    "            beta_1_power=beta_1_power,\n",
    "            one_minus_beta_1_t=1 - beta_1_t,\n",
    "            beta_2_t=beta_2_t,\n",
    "            beta_2_power=beta_2_power,\n",
    "            one_minus_beta_2_t=1 - beta_2_t))\n",
    "\n",
    "  def set_weights(self, weights):\n",
    "    params = self.weights\n",
    "    # If the weights are generated by Keras V1 optimizer, it includes vhats\n",
    "    # even without amsgrad, i.e, V1 optimizer has 3x + 1 variables, while V2\n",
    "    # optimizer has 2x + 1 variables. Filter vhats out for compatibility.\n",
    "    num_vars = int((len(params) - 1) / 2)\n",
    "    if len(weights) == 3 * num_vars + 1:\n",
    "      weights = weights[:len(params)]\n",
    "    super(Adam, self).set_weights(weights)\n",
    "\n",
    "  def _resource_apply_dense(self, grad, var, apply_state=None):\n",
    "    var_device, var_dtype = var.device, var.dtype.base_dtype\n",
    "    coefficients = ((apply_state or {}).get((var_device, var_dtype))\n",
    "                    or self._fallback_apply_state(var_device, var_dtype))\n",
    "\n",
    "    m = self.get_slot(var, 'm')\n",
    "    v = self.get_slot(var, 'v')\n",
    "\n",
    "    if not self.amsgrad:\n",
    "      return tf.raw_ops.ResourceApplyAdam(\n",
    "          var=var.handle,\n",
    "          m=m.handle,\n",
    "          v=v.handle,\n",
    "          beta1_power=coefficients['beta_1_power'],\n",
    "          beta2_power=coefficients['beta_2_power'],\n",
    "          lr=coefficients['lr_t'],\n",
    "          beta1=coefficients['beta_1_t'],\n",
    "          beta2=coefficients['beta_2_t'],\n",
    "          epsilon=coefficients['epsilon'],\n",
    "          grad=grad,\n",
    "          use_locking=self._use_locking)\n",
    "    else:\n",
    "      vhat = self.get_slot(var, 'vhat')\n",
    "      return tf.raw_ops.ResourceApplyAdamWithAmsgrad(\n",
    "          var=var.handle,\n",
    "          m=m.handle,\n",
    "          v=v.handle,\n",
    "          vhat=vhat.handle,\n",
    "          beta1_power=coefficients['beta_1_power'],\n",
    "          beta2_power=coefficients['beta_2_power'],\n",
    "          lr=coefficients['lr_t'],\n",
    "          beta1=coefficients['beta_1_t'],\n",
    "          beta2=coefficients['beta_2_t'],\n",
    "          epsilon=coefficients['epsilon'],\n",
    "          grad=grad,\n",
    "          use_locking=self._use_locking)\n",
    "\n",
    "  def _resource_apply_sparse(self, grad, var, indices, apply_state=None):\n",
    "    var_device, var_dtype = var.device, var.dtype.base_dtype\n",
    "    coefficients = ((apply_state or {}).get((var_device, var_dtype))\n",
    "                    or self._fallback_apply_state(var_device, var_dtype))\n",
    "\n",
    "    # m_t = beta1 * m + (1 - beta1) * g_t\n",
    "    m = self.get_slot(var, 'm')\n",
    "    m_scaled_g_values = grad * coefficients['one_minus_beta_1_t']\n",
    "    m_t = tf.compat.v1.assign(m, m * coefficients['beta_1_t'],\n",
    "                           use_locking=self._use_locking)\n",
    "    with tf.control_dependencies([m_t]):\n",
    "      m_t = self._resource_scatter_add(m, indices, m_scaled_g_values)\n",
    "\n",
    "    # v_t = beta2 * v + (1 - beta2) * (g_t * g_t)\n",
    "    v = self.get_slot(var, 'v')\n",
    "    v_scaled_g_values = (grad * grad) * coefficients['one_minus_beta_2_t']\n",
    "    v_t = tf.compat.v1.assign(v, v * coefficients['beta_2_t'],\n",
    "                           use_locking=self._use_locking)\n",
    "    with tf.control_dependencies([v_t]):\n",
    "      v_t = self._resource_scatter_add(v, indices, v_scaled_g_values)\n",
    "\n",
    "    if not self.amsgrad:\n",
    "      v_sqrt = tf.sqrt(v_t)\n",
    "      var_update = tf.compat.v1.assign_sub(\n",
    "          var, coefficients['lr'] * m_t / (v_sqrt + coefficients['epsilon']),\n",
    "          use_locking=self._use_locking)\n",
    "      return tf.group(*[var_update, m_t, v_t])\n",
    "    else:\n",
    "      v_hat = self.get_slot(var, 'vhat')\n",
    "      v_hat_t = tf.maximum(v_hat, v_t)\n",
    "      with tf.control_dependencies([v_hat_t]):\n",
    "        v_hat_t = tf.compat.v1.assign(\n",
    "            v_hat, v_hat_t, use_locking=self._use_locking)\n",
    "      v_hat_sqrt = tf.sqrt(v_hat_t)\n",
    "      var_update = tf.compat.v1.assign_sub(\n",
    "          var,\n",
    "          coefficients['lr'] * m_t / (v_hat_sqrt + coefficients['epsilon']),\n",
    "          use_locking=self._use_locking)\n",
    "      return tf.group(*[var_update, m_t, v_t, v_hat_t])\n",
    "\n",
    "  def get_config(self):\n",
    "    config = super(Adam, self).get_config()\n",
    "    config.update({\n",
    "        'learning_rate': self._serialize_hyperparameter('learning_rate'),\n",
    "        'decay': self._initial_decay,\n",
    "        'beta_1': self._serialize_hyperparameter('beta_1'),\n",
    "        'beta_2': self._serialize_hyperparameter('beta_2'),\n",
    "        'epsilon': self.epsilon,\n",
    "        'amsgrad': self.amsgrad,\n",
    "    })\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c45fa9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix: (60000, 28, 28)\n",
      "Target matrix: (10000, 28, 28)\n",
      "Feature matrix: (60000,)\n",
      "Target matrix: (10000,)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Cast the records into float values\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "  \n",
    "# normalize image pixel values by dividing \n",
    "# by 255\n",
    "gray_scale = 255\n",
    "x_train /= gray_scale\n",
    "x_test /= gray_scale\n",
    "\n",
    "print(\"Feature matrix:\", x_train.shape)\n",
    "print(\"Target matrix:\", x_test.shape)\n",
    "print(\"Feature matrix:\", y_train.shape)\n",
    "print(\"Target matrix:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "884b204d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-10 22:00:27.272883: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-01-10 22:00:27.272915: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-01-10 22:00:27.272935: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ruslan-Inspiron-5770): /proc/driver/nvidia/version does not exist\n",
      "2023-01-10 22:00:27.273335: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "24/24 [==============================] - 2s 59ms/step - loss: 0.5427 - accuracy: 0.8361 - val_loss: 0.4597 - val_accuracy: 0.8708\n",
      "Epoch 2/20\n",
      "24/24 [==============================] - 1s 50ms/step - loss: 0.2266 - accuracy: 0.9296 - val_loss: 0.2475 - val_accuracy: 0.9273\n",
      "Epoch 3/20\n",
      "24/24 [==============================] - 1s 50ms/step - loss: 0.1693 - accuracy: 0.9488 - val_loss: 0.2766 - val_accuracy: 0.9315\n",
      "Epoch 4/20\n",
      "24/24 [==============================] - 1s 50ms/step - loss: 0.1376 - accuracy: 0.9580 - val_loss: 0.2273 - val_accuracy: 0.9466\n",
      "Epoch 5/20\n",
      "24/24 [==============================] - 1s 50ms/step - loss: 0.1178 - accuracy: 0.9638 - val_loss: 0.1840 - val_accuracy: 0.9564\n",
      "Epoch 6/20\n",
      "24/24 [==============================] - 1s 50ms/step - loss: 0.1043 - accuracy: 0.9679 - val_loss: 0.1665 - val_accuracy: 0.9603\n",
      "Epoch 7/20\n",
      "24/24 [==============================] - 1s 51ms/step - loss: 0.0882 - accuracy: 0.9722 - val_loss: 0.1472 - val_accuracy: 0.9647\n",
      "Epoch 8/20\n",
      "24/24 [==============================] - 1s 49ms/step - loss: 0.0786 - accuracy: 0.9751 - val_loss: 0.1177 - val_accuracy: 0.9701\n",
      "Epoch 9/20\n",
      "24/24 [==============================] - 1s 54ms/step - loss: 0.0720 - accuracy: 0.9770 - val_loss: 0.1034 - val_accuracy: 0.9737\n",
      "Epoch 10/20\n",
      "24/24 [==============================] - 1s 58ms/step - loss: 0.0638 - accuracy: 0.9799 - val_loss: 0.0932 - val_accuracy: 0.9754\n",
      "Epoch 11/20\n",
      "24/24 [==============================] - 1s 56ms/step - loss: 0.0587 - accuracy: 0.9806 - val_loss: 0.0876 - val_accuracy: 0.9774\n",
      "Epoch 12/20\n",
      "24/24 [==============================] - 1s 54ms/step - loss: 0.0528 - accuracy: 0.9831 - val_loss: 0.0844 - val_accuracy: 0.9787\n",
      "Epoch 13/20\n",
      "24/24 [==============================] - 1s 52ms/step - loss: 0.0476 - accuracy: 0.9850 - val_loss: 0.0808 - val_accuracy: 0.9793\n",
      "Epoch 14/20\n",
      "24/24 [==============================] - 1s 54ms/step - loss: 0.0428 - accuracy: 0.9863 - val_loss: 0.0784 - val_accuracy: 0.9796\n",
      "Epoch 15/20\n",
      "24/24 [==============================] - 1s 52ms/step - loss: 0.0402 - accuracy: 0.9870 - val_loss: 0.0766 - val_accuracy: 0.9800\n",
      "Epoch 16/20\n",
      "24/24 [==============================] - 1s 55ms/step - loss: 0.0386 - accuracy: 0.9875 - val_loss: 0.0773 - val_accuracy: 0.9805\n",
      "Epoch 17/20\n",
      "24/24 [==============================] - 1s 54ms/step - loss: 0.0336 - accuracy: 0.9888 - val_loss: 0.0783 - val_accuracy: 0.9822\n",
      "Epoch 18/20\n",
      "24/24 [==============================] - 1s 54ms/step - loss: 0.0307 - accuracy: 0.9900 - val_loss: 0.0791 - val_accuracy: 0.9811\n",
      "Epoch 19/20\n",
      "24/24 [==============================] - 1s 55ms/step - loss: 0.0292 - accuracy: 0.9908 - val_loss: 0.0787 - val_accuracy: 0.9813\n",
      "Epoch 20/20\n",
      "24/24 [==============================] - 1s 51ms/step - loss: 0.0286 - accuracy: 0.9911 - val_loss: 0.0784 - val_accuracy: 0.9805\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    \n",
    "     # reshape 28 row * 28 column data to 28*28 rows\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    \n",
    "      # dense layer 1\n",
    "    Dense(512, activation='relu'),\n",
    "    BatchNormalization(),  \n",
    "    Dropout(0.2),\n",
    "    # dense layer 2\n",
    "    Dense(128, activation='relu'), \n",
    "    BatchNormalization(),  \n",
    "    Dropout(0.2),\n",
    "      # output layer\n",
    "    Dense(10, activation='softmax'), \n",
    "])\n",
    "\n",
    "optimizer = RLFGD(learning_rate = 0.075, momentum = 0.99, nesterov = True)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=20, \n",
    "          batch_size=2000, \n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af5886ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "24/24 [==============================] - 2s 70ms/step - loss: 0.6109 - accuracy: 0.8130 - val_loss: 0.6413 - val_accuracy: 0.7937\n",
      "Epoch 2/20\n",
      "24/24 [==============================] - 1s 52ms/step - loss: 0.2451 - accuracy: 0.9250 - val_loss: 0.3093 - val_accuracy: 0.9046\n",
      "Epoch 3/20\n",
      "24/24 [==============================] - 1s 61ms/step - loss: 0.1831 - accuracy: 0.9447 - val_loss: 0.2520 - val_accuracy: 0.9272\n",
      "Epoch 4/20\n",
      "24/24 [==============================] - 1s 53ms/step - loss: 0.1481 - accuracy: 0.9550 - val_loss: 0.2260 - val_accuracy: 0.9402\n",
      "Epoch 5/20\n",
      "24/24 [==============================] - 1s 54ms/step - loss: 0.1265 - accuracy: 0.9623 - val_loss: 0.1683 - val_accuracy: 0.9555\n",
      "Epoch 6/20\n",
      "24/24 [==============================] - 1s 54ms/step - loss: 0.1047 - accuracy: 0.9669 - val_loss: 0.1501 - val_accuracy: 0.9613\n",
      "Epoch 7/20\n",
      "24/24 [==============================] - 1s 54ms/step - loss: 0.0930 - accuracy: 0.9708 - val_loss: 0.1319 - val_accuracy: 0.9658\n",
      "Epoch 8/20\n",
      "24/24 [==============================] - 1s 52ms/step - loss: 0.0852 - accuracy: 0.9732 - val_loss: 0.1225 - val_accuracy: 0.9674\n",
      "Epoch 9/20\n",
      "24/24 [==============================] - 1s 52ms/step - loss: 0.0768 - accuracy: 0.9763 - val_loss: 0.1058 - val_accuracy: 0.9720\n",
      "Epoch 10/20\n",
      "24/24 [==============================] - 1s 52ms/step - loss: 0.0679 - accuracy: 0.9785 - val_loss: 0.0947 - val_accuracy: 0.9737\n",
      "Epoch 11/20\n",
      "24/24 [==============================] - 1s 56ms/step - loss: 0.0592 - accuracy: 0.9807 - val_loss: 0.0866 - val_accuracy: 0.9765\n",
      "Epoch 12/20\n",
      "24/24 [==============================] - 1s 54ms/step - loss: 0.0540 - accuracy: 0.9824 - val_loss: 0.0816 - val_accuracy: 0.9776\n",
      "Epoch 13/20\n",
      "24/24 [==============================] - 2s 63ms/step - loss: 0.0513 - accuracy: 0.9841 - val_loss: 0.0788 - val_accuracy: 0.9783\n",
      "Epoch 14/20\n",
      "24/24 [==============================] - 1s 53ms/step - loss: 0.0465 - accuracy: 0.9846 - val_loss: 0.0784 - val_accuracy: 0.9783\n",
      "Epoch 15/20\n",
      "24/24 [==============================] - 1s 51ms/step - loss: 0.0428 - accuracy: 0.9863 - val_loss: 0.0756 - val_accuracy: 0.9798\n",
      "Epoch 16/20\n",
      "24/24 [==============================] - 1s 53ms/step - loss: 0.0397 - accuracy: 0.9870 - val_loss: 0.0757 - val_accuracy: 0.9806\n",
      "Epoch 17/20\n",
      "24/24 [==============================] - 1s 53ms/step - loss: 0.0356 - accuracy: 0.9885 - val_loss: 0.0763 - val_accuracy: 0.9819\n",
      "Epoch 18/20\n",
      "24/24 [==============================] - 1s 53ms/step - loss: 0.0339 - accuracy: 0.9892 - val_loss: 0.0747 - val_accuracy: 0.9817\n",
      "Epoch 19/20\n",
      "24/24 [==============================] - 1s 53ms/step - loss: 0.0308 - accuracy: 0.9903 - val_loss: 0.0736 - val_accuracy: 0.9808\n",
      "Epoch 20/20\n",
      "24/24 [==============================] - 1s 56ms/step - loss: 0.0295 - accuracy: 0.9902 - val_loss: 0.0732 - val_accuracy: 0.9812\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    \n",
    "     # reshape 28 row * 28 column data to 28*28 rows\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    \n",
    "      # dense layer 1\n",
    "    Dense(512, activation='relu'),\n",
    "    BatchNormalization(),  \n",
    "    Dropout(0.2),\n",
    "    # dense layer 2\n",
    "    Dense(128, activation='relu'), \n",
    "    BatchNormalization(),  \n",
    "    Dropout(0.2),\n",
    "      # output layer\n",
    "    Dense(10, activation='softmax'), \n",
    "])\n",
    "\n",
    "optimizer = CFGD(learning_rate = 0.05, momentum = 0.99, nesterov = True)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history1 = model.fit(x_train, y_train, epochs=20, \n",
    "          batch_size=2000, \n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c928a729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "24/24 [==============================] - 2s 60ms/step - loss: 0.5624 - accuracy: 0.8350 - val_loss: 0.5171 - val_accuracy: 0.8393\n",
      "Epoch 2/20\n",
      "24/24 [==============================] - 1s 51ms/step - loss: 0.2303 - accuracy: 0.9297 - val_loss: 0.3882 - val_accuracy: 0.8921\n",
      "Epoch 3/20\n",
      "24/24 [==============================] - 1s 54ms/step - loss: 0.1688 - accuracy: 0.9486 - val_loss: 0.3508 - val_accuracy: 0.9189\n",
      "Epoch 4/20\n",
      "24/24 [==============================] - 1s 55ms/step - loss: 0.1399 - accuracy: 0.9581 - val_loss: 0.2548 - val_accuracy: 0.9468\n",
      "Epoch 5/20\n",
      "24/24 [==============================] - 1s 58ms/step - loss: 0.1169 - accuracy: 0.9634 - val_loss: 0.2089 - val_accuracy: 0.9548\n",
      "Epoch 6/20\n",
      "24/24 [==============================] - 1s 53ms/step - loss: 0.0997 - accuracy: 0.9695 - val_loss: 0.1604 - val_accuracy: 0.9632\n",
      "Epoch 7/20\n",
      "24/24 [==============================] - 1s 54ms/step - loss: 0.0872 - accuracy: 0.9728 - val_loss: 0.1421 - val_accuracy: 0.9665\n",
      "Epoch 8/20\n",
      "24/24 [==============================] - 1s 51ms/step - loss: 0.0766 - accuracy: 0.9755 - val_loss: 0.1243 - val_accuracy: 0.9700\n",
      "Epoch 9/20\n",
      "24/24 [==============================] - 1s 52ms/step - loss: 0.0682 - accuracy: 0.9785 - val_loss: 0.1092 - val_accuracy: 0.9729\n",
      "Epoch 10/20\n",
      "24/24 [==============================] - 1s 52ms/step - loss: 0.0610 - accuracy: 0.9801 - val_loss: 0.0973 - val_accuracy: 0.9749\n",
      "Epoch 11/20\n",
      "24/24 [==============================] - 1s 51ms/step - loss: 0.0570 - accuracy: 0.9815 - val_loss: 0.0880 - val_accuracy: 0.9767\n",
      "Epoch 12/20\n",
      "24/24 [==============================] - 1s 55ms/step - loss: 0.0530 - accuracy: 0.9825 - val_loss: 0.0851 - val_accuracy: 0.9769\n",
      "Epoch 13/20\n",
      "24/24 [==============================] - 1s 53ms/step - loss: 0.0478 - accuracy: 0.9846 - val_loss: 0.0800 - val_accuracy: 0.9790\n",
      "Epoch 14/20\n",
      "24/24 [==============================] - 1s 53ms/step - loss: 0.0452 - accuracy: 0.9855 - val_loss: 0.0799 - val_accuracy: 0.9792\n",
      "Epoch 15/20\n",
      "24/24 [==============================] - 1s 54ms/step - loss: 0.0411 - accuracy: 0.9866 - val_loss: 0.0786 - val_accuracy: 0.9792\n",
      "Epoch 16/20\n",
      "24/24 [==============================] - 1s 52ms/step - loss: 0.0386 - accuracy: 0.9870 - val_loss: 0.0766 - val_accuracy: 0.9796\n",
      "Epoch 17/20\n",
      "24/24 [==============================] - 1s 60ms/step - loss: 0.0357 - accuracy: 0.9883 - val_loss: 0.0759 - val_accuracy: 0.9794\n",
      "Epoch 18/20\n",
      "24/24 [==============================] - 1s 54ms/step - loss: 0.0338 - accuracy: 0.9890 - val_loss: 0.0754 - val_accuracy: 0.9799\n",
      "Epoch 19/20\n",
      "24/24 [==============================] - 1s 53ms/step - loss: 0.0315 - accuracy: 0.9893 - val_loss: 0.0754 - val_accuracy: 0.9804\n",
      "Epoch 20/20\n",
      "24/24 [==============================] - 1s 57ms/step - loss: 0.0296 - accuracy: 0.9900 - val_loss: 0.0760 - val_accuracy: 0.9815\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    \n",
    "     # reshape 28 row * 28 column data to 28*28 rows\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    \n",
    "      # dense layer 1\n",
    "    Dense(512, activation='relu'),\n",
    "    BatchNormalization(),  \n",
    "    Dropout(0.2),\n",
    "    # dense layer 2\n",
    "    Dense(128, activation='relu'), \n",
    "    BatchNormalization(),  \n",
    "    Dropout(0.2),\n",
    "      # output layer\n",
    "    Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "optimizer = GLFGD(learning_rate = 0.08, momentum = 0.99, nesterov = True)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history2 = model.fit(x_train, y_train, epochs=20, \n",
    "          batch_size=2000, \n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25ab45fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "24/24 [==============================] - 2s 57ms/step - loss: 0.6066 - accuracy: 0.8148 - val_loss: 0.9841 - val_accuracy: 0.7702\n",
      "Epoch 2/20\n",
      "24/24 [==============================] - 2s 73ms/step - loss: 0.2167 - accuracy: 0.9371 - val_loss: 0.8443 - val_accuracy: 0.9028\n",
      "Epoch 3/20\n",
      "24/24 [==============================] - 2s 63ms/step - loss: 0.1567 - accuracy: 0.9546 - val_loss: 0.7383 - val_accuracy: 0.9310\n",
      "Epoch 4/20\n",
      "24/24 [==============================] - 2s 62ms/step - loss: 0.1247 - accuracy: 0.9636 - val_loss: 0.6361 - val_accuracy: 0.9398\n",
      "Epoch 5/20\n",
      "24/24 [==============================] - 1s 58ms/step - loss: 0.1002 - accuracy: 0.9708 - val_loss: 0.5199 - val_accuracy: 0.9496\n",
      "Epoch 6/20\n",
      "24/24 [==============================] - 1s 60ms/step - loss: 0.0822 - accuracy: 0.9766 - val_loss: 0.4282 - val_accuracy: 0.9578\n",
      "Epoch 7/20\n",
      "24/24 [==============================] - 2s 63ms/step - loss: 0.0690 - accuracy: 0.9801 - val_loss: 0.3262 - val_accuracy: 0.9578\n",
      "Epoch 8/20\n",
      "24/24 [==============================] - 1s 60ms/step - loss: 0.0587 - accuracy: 0.9834 - val_loss: 0.2473 - val_accuracy: 0.9653\n",
      "Epoch 9/20\n",
      "24/24 [==============================] - 1s 54ms/step - loss: 0.0501 - accuracy: 0.9859 - val_loss: 0.2069 - val_accuracy: 0.9671\n",
      "Epoch 10/20\n",
      "24/24 [==============================] - 1s 54ms/step - loss: 0.0417 - accuracy: 0.9888 - val_loss: 0.1596 - val_accuracy: 0.9696\n",
      "Epoch 11/20\n",
      "24/24 [==============================] - 1s 52ms/step - loss: 0.0371 - accuracy: 0.9895 - val_loss: 0.1353 - val_accuracy: 0.9683\n",
      "Epoch 12/20\n",
      "24/24 [==============================] - 1s 52ms/step - loss: 0.0318 - accuracy: 0.9909 - val_loss: 0.1116 - val_accuracy: 0.9713\n",
      "Epoch 13/20\n",
      "24/24 [==============================] - 1s 52ms/step - loss: 0.0272 - accuracy: 0.9925 - val_loss: 0.0996 - val_accuracy: 0.9737\n",
      "Epoch 14/20\n",
      "24/24 [==============================] - 1s 52ms/step - loss: 0.0243 - accuracy: 0.9933 - val_loss: 0.0858 - val_accuracy: 0.9753\n",
      "Epoch 15/20\n",
      "24/24 [==============================] - 1s 52ms/step - loss: 0.0210 - accuracy: 0.9945 - val_loss: 0.0795 - val_accuracy: 0.9756\n",
      "Epoch 16/20\n",
      "24/24 [==============================] - 1s 53ms/step - loss: 0.0189 - accuracy: 0.9950 - val_loss: 0.0743 - val_accuracy: 0.9783\n",
      "Epoch 17/20\n",
      "24/24 [==============================] - 1s 53ms/step - loss: 0.0165 - accuracy: 0.9956 - val_loss: 0.0758 - val_accuracy: 0.9769\n",
      "Epoch 18/20\n",
      "24/24 [==============================] - 1s 53ms/step - loss: 0.0158 - accuracy: 0.9961 - val_loss: 0.0740 - val_accuracy: 0.9768\n",
      "Epoch 19/20\n",
      "24/24 [==============================] - 1s 53ms/step - loss: 0.0146 - accuracy: 0.9960 - val_loss: 0.0734 - val_accuracy: 0.9772\n",
      "Epoch 20/20\n",
      "24/24 [==============================] - 1s 61ms/step - loss: 0.0137 - accuracy: 0.9963 - val_loss: 0.0770 - val_accuracy: 0.9771\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    \n",
    "     # reshape 28 row * 28 column data to 28*28 rows\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    \n",
    "      # dense layer 1\n",
    "    Dense(512, activation='relu'),\n",
    "    BatchNormalization(),  \n",
    "    Dropout(0.2),\n",
    "    # dense layer 2\n",
    "    Dense(128, activation='relu'), \n",
    "    BatchNormalization(),  \n",
    "    Dropout(0.2),\n",
    "      # output layer\n",
    "    Dense(10, activation='softmax'),  \n",
    "])\n",
    "\n",
    "optimizer = Adam(learning_rate = 0.001)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history3 = model.fit(x_train, y_train, epochs=20, \n",
    "          batch_size=2000, \n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e82e440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "24/24 [==============================] - 2s 58ms/step - loss: 0.9733 - accuracy: 0.7030 - val_loss: 0.9364 - val_accuracy: 0.7168\n",
      "Epoch 2/20\n",
      "24/24 [==============================] - 1s 54ms/step - loss: 0.3672 - accuracy: 0.8903 - val_loss: 0.6808 - val_accuracy: 0.7855\n",
      "Epoch 3/20\n",
      "24/24 [==============================] - 1s 57ms/step - loss: 0.2895 - accuracy: 0.9124 - val_loss: 0.3675 - val_accuracy: 0.8932\n",
      "Epoch 4/20\n",
      "24/24 [==============================] - 1s 52ms/step - loss: 0.2419 - accuracy: 0.9267 - val_loss: 0.2580 - val_accuracy: 0.9223\n",
      "Epoch 5/20\n",
      "24/24 [==============================] - 1s 53ms/step - loss: 0.2084 - accuracy: 0.9369 - val_loss: 0.2063 - val_accuracy: 0.9381\n",
      "Epoch 6/20\n",
      "24/24 [==============================] - 1s 57ms/step - loss: 0.1795 - accuracy: 0.9453 - val_loss: 0.1801 - val_accuracy: 0.9441\n",
      "Epoch 7/20\n",
      "24/24 [==============================] - 1s 53ms/step - loss: 0.1595 - accuracy: 0.9510 - val_loss: 0.1600 - val_accuracy: 0.9517\n",
      "Epoch 8/20\n",
      "24/24 [==============================] - 1s 54ms/step - loss: 0.1406 - accuracy: 0.9562 - val_loss: 0.1413 - val_accuracy: 0.9583\n",
      "Epoch 9/20\n",
      "24/24 [==============================] - 1s 52ms/step - loss: 0.1312 - accuracy: 0.9594 - val_loss: 0.1249 - val_accuracy: 0.9628\n",
      "Epoch 10/20\n",
      "24/24 [==============================] - 1s 52ms/step - loss: 0.1172 - accuracy: 0.9639 - val_loss: 0.1137 - val_accuracy: 0.9668\n",
      "Epoch 11/20\n",
      "24/24 [==============================] - 1s 52ms/step - loss: 0.1098 - accuracy: 0.9661 - val_loss: 0.1069 - val_accuracy: 0.9683\n",
      "Epoch 12/20\n",
      "24/24 [==============================] - 1s 58ms/step - loss: 0.1009 - accuracy: 0.9686 - val_loss: 0.1011 - val_accuracy: 0.9697\n",
      "Epoch 13/20\n",
      "24/24 [==============================] - 1s 52ms/step - loss: 0.0939 - accuracy: 0.9710 - val_loss: 0.0957 - val_accuracy: 0.9717\n",
      "Epoch 14/20\n",
      "24/24 [==============================] - 1s 56ms/step - loss: 0.0862 - accuracy: 0.9725 - val_loss: 0.0908 - val_accuracy: 0.9730\n",
      "Epoch 15/20\n",
      "24/24 [==============================] - 1s 52ms/step - loss: 0.0814 - accuracy: 0.9743 - val_loss: 0.0876 - val_accuracy: 0.9743\n",
      "Epoch 16/20\n",
      "24/24 [==============================] - 1s 56ms/step - loss: 0.0752 - accuracy: 0.9754 - val_loss: 0.0856 - val_accuracy: 0.9739\n",
      "Epoch 17/20\n",
      "24/24 [==============================] - 1s 51ms/step - loss: 0.0727 - accuracy: 0.9773 - val_loss: 0.0835 - val_accuracy: 0.9750\n",
      "Epoch 18/20\n",
      "24/24 [==============================] - 1s 52ms/step - loss: 0.0678 - accuracy: 0.9785 - val_loss: 0.0812 - val_accuracy: 0.9765\n",
      "Epoch 19/20\n",
      "24/24 [==============================] - 2s 65ms/step - loss: 0.0613 - accuracy: 0.9805 - val_loss: 0.0796 - val_accuracy: 0.9773\n",
      "Epoch 20/20\n",
      "24/24 [==============================] - 1s 58ms/step - loss: 0.0612 - accuracy: 0.9805 - val_loss: 0.0782 - val_accuracy: 0.9774\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    \n",
    "     # reshape 28 row * 28 column data to 28*28 rows\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    \n",
    "      # dense layer 1\n",
    "    Dense(512, activation='relu'),\n",
    "    BatchNormalization(),  \n",
    "    Dropout(0.2),\n",
    "    # dense layer 2\n",
    "    Dense(128, activation='relu'), \n",
    "    BatchNormalization(),  \n",
    "    Dropout(0.2),\n",
    "      # output layer\n",
    "    Dense(10, activation='softmax'),  \n",
    "])\n",
    "\n",
    "optimizer = SGD(learning_rate = 0.01, momentum = 0.99, nesterov = True)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history4 = model.fit(x_train, y_train, epochs=20, \n",
    "          batch_size=2000, \n",
    "          validation_split=0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
